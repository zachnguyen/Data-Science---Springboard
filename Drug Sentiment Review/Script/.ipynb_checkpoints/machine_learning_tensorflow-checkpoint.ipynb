{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore deprecation warnings in sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data directory\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()),'Data')\n",
    "\n",
    "# Set model directory\n",
    "\n",
    "model_dir = os.path.join(os.path.dirname(os.getcwd()), 'Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data paths\n",
    "\n",
    "train_path = os.path.join(data_dir, 'train.csv')\n",
    "\n",
    "train_processed_path = os.path.join(data_dir, 'interim', 'train_preprocessed.txt')\n",
    "\n",
    "dense_feat_path = os.path.join(data_dir, 'interim', 'dense_feat.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "train_processed = pd.read_json(train_processed_path)\n",
    "dense_feat = pd.read_json(dense_feat_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Define X and y\n",
    "\n",
    "X = train_processed.text\n",
    "y = train_processed.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Split train and test set\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def elmo_vectors(x):\n",
    "  embeddings = elmo([x], signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    # return average of ELMo features\n",
    "    return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elmo_train = [elmo_vectors(row['text']) for index, row in train_processed.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN (example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabulary_size = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old\n",
    "\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('---review---')\n",
    "print(X_train[6])\n",
    "print('---label---')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in X_train[6]])\n",
    "print('---label---')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Maximum review length: {}'.format(len(max((X_train + X_test), key=len))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Minimum review length: {}'.format(len(min((X_test + X_test), key=len))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(model_dir,\"rnn_imdb_example_model.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join(model_dir,\"rnn_imdb_example_model.h5\"))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=10)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = train_processed[train_processed.sentiment == 0][0:200]\n",
    "c1 = train_processed[train_processed.sentiment == 1][0:200]\n",
    "c2 = train_processed[train_processed.sentiment == 2][0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = pd.concat([c0, c1, c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(train_sample['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(train_sample['text'].values)\n",
    "X = pad_sequences(X, maxlen = max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(train_sample['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 100) (450, 3)\n",
      "(150, 100) (150, 3)\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, random_state = 42)\n",
    "print(Xtrain.shape,ytrain.shape)\n",
    "print(Xtest.shape,ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs = {}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict, average = 'macro')\n",
    "        _val_recall = recall_score(val_targ, val_predict, average = 'macro')\n",
    "        _val_precision = precision_score(val_targ, val_predict, average = 'macro')\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\"- val_f1: %f - val_precision: %f - val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "metrics = Metrics()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_macro_score(y_true, y_pred):\n",
    "    precision_0 = K.sum((K.round(y_pred) * y_true)[:,0]) / (K.sum(K.round(y_pred)[:,0]) + K.epsilon())\n",
    "    precision_1 = K.sum((K.round(y_pred) * y_true)[:,1]) / (K.sum(K.round(y_pred)[:,1]) + K.epsilon())\n",
    "    precision_2 = K.sum((K.round(y_pred) * y_true)[:,2]) / (K.sum(K.round(y_pred)[:,2]) + K.epsilon())\n",
    "    \n",
    "    recall_0 = K.sum((K.round(y_pred) * y_true)[:,0]) / (K.sum(y_true[:,0]) + K.epsilon())\n",
    "    recall_1 = K.sum((K.round(y_pred) * y_true)[:,1]) / (K.sum(y_true[:,1]) + K.epsilon())\n",
    "    recall_2 = K.sum((K.round(y_pred) * y_true)[:,2]) / (K.sum(y_true[:,2]) + K.epsilon())\n",
    "    \n",
    "    f1_0 = (2 * precision_0 * recall_0) / (precision_0 + recall_0 + K.epsilon())\n",
    "    f1_1 = (2 * precision_1 * recall_1) / (precision_1 + recall_1 + K.epsilon())\n",
    "    f1_2 = (2 * precision_2 * recall_2) / (precision_2 + recall_2 + K.epsilon())\n",
    "    \n",
    "    return K.mean(precision_0, precision_1, precision_2), K.mean(recall_0, recall_1, recall_2), K.mean(f1_0, f1_1, f1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_class_accuracy(y_true, y_pred):\n",
    "    INTERESTING_CLASS_ID = 0\n",
    "    class_id_true = K.argmax(y_true, axis=-1)\n",
    "    class_id_preds = K.argmax(y_pred, axis=-1)\n",
    "    accuracy_mask = K.cast(K.equal(class_id_preds, INTERESTING_CLASS_ID), 'int32')\n",
    "    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask\n",
    "    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
    "    return class_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import functools\n",
    "from itertools import product\n",
    "\n",
    "def w_categorical_crossentropy(y_true, y_pred, weights):\n",
    "    nb_cl = len(weights)\n",
    "    final_mask = K.zeros_like(y_pred[:, 0])\n",
    "    y_pred_max = K.max(y_pred, axis=1)\n",
    "    y_pred_max = K.expand_dims(y_pred_max, 1)\n",
    "    y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "    for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "\n",
    "        final_mask += (K.cast(weights[c_t, c_p],K.floatx()) * K.cast(y_pred_max_mat[:, c_p] ,K.floatx())* K.cast(y_true[:, c_t],K.floatx()))\n",
    "    return K.categorical_crossentropy(y_pred, y_true) * final_mask\n",
    "w_array = np.ones((3,3))\n",
    "w_array[2,0] = 1.5\n",
    "w_array[1,0] = 1.5\n",
    "ncce = functools.partial(w_categorical_crossentropy, weights=w_array)\n",
    "#ncce.__name__ ='w_categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def focal_loss(y_true, y_pred):\n",
    "    gamma = 2.0 \n",
    "    alpha = 0.25\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    precision_0 = K.sum((K.round(y_pred) * y_true)[:,0]) / (K.sum(K.round(y_pred)[:,0]) + K.epsilon())\n",
    "    precision_1 = K.sum((K.round(y_pred) * y_true)[:,1]) / (K.sum(K.round(y_pred)[:,1]) + K.epsilon())\n",
    "    precision_2 = K.sum((K.round(y_pred) * y_true)[:,2]) / (K.sum(K.round(y_pred)[:,2]) + K.epsilon())\n",
    "    \n",
    "    recall_0 = K.sum((K.round(y_pred) * y_true)[:,0]) / (K.sum(y_true[:,0]) + K.epsilon())\n",
    "    recall_1 = K.sum((K.round(y_pred) * y_true)[:,1]) / (K.sum(y_true[:,1]) + K.epsilon())\n",
    "    recall_2 = K.sum((K.round(y_pred) * y_true)[:,2]) / (K.sum(y_true[:,2]) + K.epsilon())\n",
    "    \n",
    "    f1_0 = (2 * precision_0 * recall_0) / (precision_0 + recall_0 + K.epsilon())\n",
    "    f1_1 = (2 * precision_1 * recall_1) / (precision_1 + recall_1 + K.epsilon())\n",
    "    f1_2 = (2 * precision_2 * recall_2) / (precision_2 + recall_2 + K.epsilon())\n",
    "    \n",
    "    f1_macro = (f1_0 + f1_1 + f1_2) / 3\n",
    "    return (1-f1_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "class_weights = compute_sample_weight('balanced', \n",
    "                                      np.unique(train_processed.sentiment),\n",
    "                                      train_processed.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_weights = {0: 1/sum(ytrain[:,0]),\n",
    "                 1: 1/sum(ytrain[:,1]),\n",
    "                 2: 1/sum(ytrain[:,2])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From Z:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From Z:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From Z:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From Z:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From Z:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From Z:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From Z:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 32)           3200      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 56,703\n",
      "Trainable params: 56,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32\n",
    "lstm_out = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, \n",
    "                    embed_dim, \n",
    "                    input_length = X.shape[1], \n",
    "                    dropout=0.2))\n",
    "model.add(LSTM(lstm_out, \n",
    "               dropout_U=0.2,\n",
    "               dropout_W=0.2))\n",
    "model.add(Dense(3,\n",
    "                activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', sensitivity, specificity])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From Z:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 386 samples, validate on 64 samples\n",
      "Epoch 1/20\n",
      "386/386 [==============================] - 8s 21ms/step - loss: 1.0992 - acc: 0.3187 - sensitivity: 0.0000e+00 - specificity: 1.0000 - val_loss: 1.0983 - val_acc: 0.4219 - val_sensitivity: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 2/20\n",
      "386/386 [==============================] - 3s 7ms/step - loss: 1.0973 - acc: 0.3705 - sensitivity: 0.0000e+00 - specificity: 1.0000 - val_loss: 1.0988 - val_acc: 0.3594 - val_sensitivity: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 3/20\n",
      "386/386 [==============================] - 2s 6ms/step - loss: 1.0948 - acc: 0.4171 - sensitivity: 0.0000e+00 - specificity: 1.0000 - val_loss: 1.1013 - val_acc: 0.2656 - val_sensitivity: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 4/20\n",
      "386/386 [==============================] - 3s 7ms/step - loss: 1.0928 - acc: 0.4145 - sensitivity: 0.0000e+00 - specificity: 1.0000 - val_loss: 1.0991 - val_acc: 0.3281 - val_sensitivity: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 5/20\n",
      "386/386 [==============================] - 2s 6ms/step - loss: 1.0890 - acc: 0.4870 - sensitivity: 0.0000e+00 - specificity: 1.0000 - val_loss: 1.0936 - val_acc: 0.3906 - val_sensitivity: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 6/20\n",
      "386/386 [==============================] - 3s 7ms/step - loss: 1.0827 - acc: 0.4922 - sensitivity: 0.0000e+00 - specificity: 1.0000 - val_loss: 1.0896 - val_acc: 0.4375 - val_sensitivity: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 7/20\n",
      "386/386 [==============================] - 3s 8ms/step - loss: 1.0790 - acc: 0.4896 - sensitivity: 0.0000e+00 - specificity: 1.0000 - val_loss: 1.0887 - val_acc: 0.3906 - val_sensitivity: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 8/20\n",
      "386/386 [==============================] - 2s 6ms/step - loss: 1.0731 - acc: 0.5000 - sensitivity: 0.0000e+00 - specificity: 1.0000 - val_loss: 1.0856 - val_acc: 0.4062 - val_sensitivity: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 9/20\n",
      "386/386 [==============================] - 2s 6ms/step - loss: 1.0655 - acc: 0.5233 - sensitivity: 0.0000e+00 - specificity: 1.0000 - val_loss: 1.0833 - val_acc: 0.4062 - val_sensitivity: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 10/20\n",
      "386/386 [==============================] - 4s 9ms/step - loss: 1.0467 - acc: 0.5363 - sensitivity: 0.0078 - specificity: 1.0000 - val_loss: 1.0708 - val_acc: 0.4062 - val_sensitivity: 0.0156 - val_specificity: 0.9844\n",
      "Epoch 11/20\n",
      "386/386 [==============================] - 4s 10ms/step - loss: 1.0366 - acc: 0.5000 - sensitivity: 0.0466 - specificity: 0.9819 - val_loss: 1.0715 - val_acc: 0.4375 - val_sensitivity: 0.0312 - val_specificity: 0.9922\n",
      "Epoch 12/20\n",
      "386/386 [==============================] - 5s 12ms/step - loss: 1.0117 - acc: 0.5181 - sensitivity: 0.0933 - specificity: 0.9883 - val_loss: 1.0728 - val_acc: 0.4062 - val_sensitivity: 0.0625 - val_specificity: 0.9766\n",
      "Epoch 13/20\n",
      "386/386 [==============================] - 3s 7ms/step - loss: 0.9939 - acc: 0.5104 - sensitivity: 0.1865 - specificity: 0.9430 - val_loss: 1.0824 - val_acc: 0.4688 - val_sensitivity: 0.1406 - val_specificity: 0.9062\n",
      "Epoch 14/20\n",
      "386/386 [==============================] - 5s 12ms/step - loss: 0.9763 - acc: 0.5130 - sensitivity: 0.2617 - specificity: 0.9223 - val_loss: 1.1010 - val_acc: 0.3750 - val_sensitivity: 0.1094 - val_specificity: 0.9375\n",
      "Epoch 15/20\n",
      "386/386 [==============================] - 3s 9ms/step - loss: 0.9719 - acc: 0.5415 - sensitivity: 0.1528 - specificity: 0.9767 - val_loss: 1.1111 - val_acc: 0.3594 - val_sensitivity: 0.0625 - val_specificity: 0.9766\n",
      "Epoch 16/20\n",
      "386/386 [==============================] - 3s 8ms/step - loss: 0.9605 - acc: 0.5492 - sensitivity: 0.1710 - specificity: 0.9767 - val_loss: 1.1117 - val_acc: 0.3438 - val_sensitivity: 0.0938 - val_specificity: 0.9531\n",
      "Epoch 17/20\n",
      "386/386 [==============================] - 3s 8ms/step - loss: 0.9410 - acc: 0.5570 - sensitivity: 0.2150 - specificity: 0.9547 - val_loss: 1.1019 - val_acc: 0.3750 - val_sensitivity: 0.1250 - val_specificity: 0.9297\n",
      "Epoch 18/20\n",
      "386/386 [==============================] - 3s 7ms/step - loss: 0.9212 - acc: 0.5699 - sensitivity: 0.2409 - specificity: 0.9508 - val_loss: 1.1161 - val_acc: 0.3906 - val_sensitivity: 0.1406 - val_specificity: 0.8906\n",
      "Epoch 19/20\n",
      "386/386 [==============================] - 3s 8ms/step - loss: 0.9127 - acc: 0.5492 - sensitivity: 0.3264 - specificity: 0.9288 - val_loss: 1.1569 - val_acc: 0.3281 - val_sensitivity: 0.1250 - val_specificity: 0.8125\n",
      "Epoch 20/20\n",
      "386/386 [==============================] - 3s 7ms/step - loss: 0.8956 - acc: 0.5751 - sensitivity: 0.3316 - specificity: 0.9352 - val_loss: 1.1666 - val_acc: 0.3125 - val_sensitivity: 0.1719 - val_specificity: 0.8203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb61f078d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "Xvalid, yvalid = Xtrain[:batch_size], ytrain[:batch_size]\n",
    "Xtrain1, ytrain1 = Xtrain[batch_size:], ytrain[batch_size:]\n",
    "model.fit(Xtrain1, ytrain1, \n",
    "          validation_data=(Xvalid, yvalid), \n",
    "          batch_size=batch_size, \n",
    "          epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1756530237197875,\n",
       " 0.3266666676600774,\n",
       " 0.17333333392937977,\n",
       " 0.8633333349227905]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(Xtest, ytest, verbose=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_df = pd.DataFrame(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.440429\n",
       "1      0.738290\n",
       "2      0.562642\n",
       "3      0.404493\n",
       "4      0.428722\n",
       "         ...   \n",
       "145    0.394123\n",
       "146    0.395079\n",
       "147    0.789879\n",
       "148    0.696679\n",
       "149    0.540615\n",
       "Length: 150, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred_max = ypred_df.apply(max, axis = 1)\n",
    "ypred_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in ypred_df.iterrows():\n",
    "    for label, item in row.items():\n",
    "        if item == ypred_max[index]:\n",
    "            row[label] = 1\n",
    "        else:\n",
    "            row[label] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2\n",
       "0    1.0  0.0  0.0\n",
       "1    0.0  0.0  1.0\n",
       "2    0.0  0.0  1.0\n",
       "3    0.0  0.0  1.0\n",
       "4    0.0  0.0  1.0\n",
       "..   ...  ...  ...\n",
       "145  0.0  1.0  0.0\n",
       "146  0.0  0.0  1.0\n",
       "147  0.0  1.0  0.0\n",
       "148  0.0  0.0  1.0\n",
       "149  0.0  0.0  1.0\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x = [0,1,2],\n",
    "       height = [np.sum(np.round(ypred_df[0])), np.sum(np.round(ypred_df[1])), np.sum(np.round(ypred_df[2]))])\n",
    "plt.xticks([0,1,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.321078431372549"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ytest, ypred_df, average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ypred_tensor = tf.convert_to_tensor(ypred,tf.float64)\n",
    "ytest_tensor = tf.convert_to_tensor(ytest,tf.float64)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(tensor[:,1]))\n",
    "\n",
    "sess.run(K.sum((K.round(ypred_tensor) * ytest_tensor)[:,2]) / (K.sum(K.round(ypred_tensor)[:,2]) + K.epsilon()))\n",
    "\n",
    "sess.run\n",
    "\n",
    "sess.run(K.sum((K.round(ypred_tensor) *ytest_tensor)[:,0]) / (K.sum(ytest_tensor[:,0]) + K.epsilon()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
